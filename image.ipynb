{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import insightface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thaivu/.pyenv/versions/3.10.2/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:54: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/thaivu/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/thaivu/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/thaivu/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/thaivu/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/thaivu/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    }
   ],
   "source": [
    "face_model = insightface.app.FaceAnalysis()\n",
    "face_model.prepare(ctx_id=-1, det_size=(640, 640))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import paths, resize\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_mapping = {}\n",
    "name_mapping = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "\n",
    "f = 512  # num of dimensions\n",
    "annoy = AnnoyIndex(f, 'angular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "DETECT_THRESHOLD = 0.6\n",
    "\n",
    "def embedding_extract(imgs_folder, id2name):\n",
    "    global id_mapping, name_mapping\n",
    "    \n",
    "    id_mapping = {}\n",
    "    name_mapping = id2name\n",
    "    count = 0\n",
    "\n",
    "    for img_path in paths.list_images(imgs_folder):\n",
    "        try:\n",
    "            dossier_id = img_path.split(os.path.sep)[-2]\n",
    "            print(f\"[DEBG] dossier_id: {dossier_id}\")\n",
    "            img = cv2.imread(img_path)\n",
    "            # TODO resize\n",
    "            img = resize(img, width=250)\n",
    "            faces = face_model.get(img)\n",
    "            if len(faces) == 1:\n",
    "                for face in faces:\n",
    "                    if face.det_score < DETECT_THRESHOLD:\n",
    "                        continue\n",
    "                    embedding = face.embedding.flatten()\n",
    "                    print(f\"[DEBUG] embedding size: {embedding.shape}\")\n",
    "                    annoy.add_item(count, embedding)\n",
    "                    id_mapping[str(count)] = dossier_id\n",
    "                    count += 1\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    annoy.build(4)\n",
    "    print(\"[DEBUG] extract successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBG] dossier_id: sakura\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thaivu/.pyenv/versions/3.10.2/lib/python3.10/site-packages/insightface/utils/transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] embedding size: (512,)\n",
      "[DEBG] dossier_id: sakura\n",
      "[DEBG] dossier_id: chaewon\n",
      "[DEBUG] embedding size: (512,)\n",
      "[DEBG] dossier_id: eunchae\n",
      "[DEBUG] embedding size: (512,)\n",
      "[DEBG] dossier_id: yunjin\n",
      "[DEBUG] embedding size: (512,)\n",
      "[DEBG] dossier_id: kazuha\n",
      "[DEBUG] embedding size: (512,)\n",
      "[DEBUG] extract successful\n"
     ]
    }
   ],
   "source": [
    "embedding_extract('lesserafim', 'lesserafim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'sakura', '1': 'chaewon', '2': 'eunchae', '3': 'yunjin', '4': 'kazuha'}\n",
      "lesserafim\n"
     ]
    }
   ],
   "source": [
    "print(id_mapping)\n",
    "print(name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from vidgear.gears import CamGear\n",
    "import uuid\n",
    "\n",
    "def analyze(filename, sub_dir, video_id):\n",
    "    \"\"\" Analyze video\n",
    "\n",
    "    Args:\n",
    "        video_path  : video full path\n",
    "        sub_dir     : where to save dossier images found\n",
    "        video_id    : ID of video\n",
    "\n",
    "    Returns:\n",
    "        results     : List<(dossier_id/name, image_path)>\n",
    "    \"\"\"\n",
    "    print(f'[DEBUG] id_mapping: {id_mapping}')\n",
    "    results = []\n",
    "    unknown_faces = []\n",
    "\n",
    "    milestone = 5\n",
    "    extra = 5\n",
    "\n",
    "    status = 'PROCESSING'\n",
    "    # before_time = time.time()\n",
    "\n",
    "    image = cv2.imread(filename)\n",
    "    height, width, _ = image.shape\n",
    "    frame_rate = 60\n",
    "    frame_id = 0\n",
    "    start_time = time.time()\n",
    "    completed_frames = 0\n",
    "\n",
    "    try:\n",
    "        completed_frames += 1\n",
    "        small_frame = cv2.resize(image, (0, 0), fx=0.25, fy=0.25)\n",
    "        faces = face_model.get(small_frame)\n",
    "        \n",
    "        if len(faces) > 0:\n",
    "            timestamp = time.time() - start_time\n",
    "            print(f\"[INFO] found {len(faces)} faces in {timestamp}\")\n",
    "                    \n",
    "        print([x.det_score for x in faces])\n",
    "        for face in faces:\n",
    "            print(f\" >> det_score: {face.det_score}\")\n",
    "            if face.det_score < 0.6:\n",
    "                continue\n",
    "            print(f\" >> found face with p: {face.det_score}\")\n",
    "\n",
    "            embedding = face.embedding.flatten()\n",
    "            closest_indices, distances = annoy.get_nns_by_vector(\n",
    "                        embedding, n=1, include_distances=True)\n",
    "            closest_embedding = annoy.get_item_vector(\n",
    "                closest_indices[0])\n",
    "            print(closest_indices)\n",
    "            similarity = (2. - distances[0] ** 2) / 2.\n",
    "            print(f\" >> sim: {similarity}\")\n",
    "\n",
    "            left, top, right, bottom = tuple(\n",
    "                        face.bbox.astype(np.int).flatten())\n",
    "            if left < 0:\n",
    "                left = 1\n",
    "            if top < 0:\n",
    "                top = 1\n",
    "            if right > width:\n",
    "                right = width - 1\n",
    "            if bottom > height:\n",
    "                bottom = height - 1\n",
    "            face_size = (right - left) * (bottom - top)\n",
    "            print(left, top, right, bottom)\n",
    "\n",
    "            if similarity >= 0.2:\n",
    "                print(f\" >> guess: {id_mapping[str(closest_indices[0])]}\")\n",
    "                crop_img = image[top:bottom, left:right]\n",
    "                crop_img = cv2.cvtColor(crop_img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                dossier_id = id_mapping[str(closest_indices[0])]\n",
    "                filename = dossier_id + '_' + \\\n",
    "                    str(uuid.uuid4()) + '.jpg'\n",
    "                path_to_save = os.path.join(\n",
    "                    sub_dir, filename)\n",
    "                cv2.imwrite(path_to_save, crop_img)\n",
    "                cv2.imshow(path_to_save, crop_img)\n",
    "                results.append(\n",
    "                    (dossier_id, path_to_save))\n",
    "    finally:\n",
    "        cv2.destroyAllWindows() \n",
    "        print(\"[INFO] finished video analysing\")\n",
    "    return results, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] id_mapping: {'0': 'sakura', '1': 'chaewon', '2': 'eunchae', '3': 'yunjin', '4': 'kazuha'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thaivu/.pyenv/versions/3.10.2/lib/python3.10/site-packages/insightface/utils/transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] found 5 faces in 1.3276469707489014\n",
      "[0.903146, 0.8850911, 0.87207186, 0.8649545, 0.809651]\n",
      " >> det_score: 0.9031460285186768\n",
      " >> found face with p: 0.9031460285186768\n",
      "[4]\n",
      " >> sim: 0.2937805731688101\n",
      "39 17 57 39\n",
      " >> guess: kazuha\n",
      " >> det_score: 0.8850911259651184\n",
      " >> found face with p: 0.8850911259651184\n",
      "[0]\n",
      " >> sim: 0.3221736485199642\n",
      "102 21 119 44\n",
      " >> guess: sakura\n",
      " >> det_score: 0.8720718622207642\n",
      " >> found face with p: 0.8720718622207642\n",
      "[2]\n",
      " >> sim: 0.42144804319011087\n",
      "172 21 189 44\n",
      " >> guess: eunchae\n",
      " >> det_score: 0.8649544715881348\n",
      " >> found face with p: 0.8649544715881348\n",
      "[3]\n",
      " >> sim: 0.34350944620461377\n",
      "132 17 149 39\n",
      " >> guess: yunjin\n",
      " >> det_score: 0.8096510171890259\n",
      " >> found face with p: 0.8096510171890259\n",
      "[1]\n",
      " >> sim: 0.29182409732987225\n",
      "70 25 87 44\n",
      " >> guess: chaewon\n",
      "[INFO] finished video analysing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_x/zckfnlk92vq72rvw9szdlzqw0000gn/T/ipykernel_31721/1539852019.py:60: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  face.bbox.astype(np.int).flatten())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('kazuha', 'dossier/kazuha_9565056d-ad22-481d-80d9-8711bfd538da.jpg'),\n",
       "  ('sakura', 'dossier/sakura_d7ea3131-2750-4c73-a035-3ad8749c9a9e.jpg'),\n",
       "  ('eunchae', 'dossier/eunchae_34dd6130-ee19-4d17-8e28-035374eadd13.jpg'),\n",
       "  ('yunjin', 'dossier/yunjin_ec2b0b9d-5acd-4ba4-8fd9-09c005aea23c.jpg'),\n",
       "  ('chaewon', 'dossier/chaewon_bbb05f43-1cc3-4746-b7bf-ac09e5fe2852.jpg')],\n",
       " 'PROCESSING')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "analyze('input_2.jpg', 'dossier', 'lesserafim')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (main, Feb 27 2022, 23:54:06) [Clang 13.0.0 (clang-1300.0.29.3)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72cbb6b4844051e8af637830c35d5a3d1fa600669ba22fdd1774ac91ab2e3c0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
