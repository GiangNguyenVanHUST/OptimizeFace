{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import insightface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thaivu/.pyenv/versions/3.10.2/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:54: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/thaivu/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/thaivu/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/thaivu/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/thaivu/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/thaivu/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    }
   ],
   "source": [
    "face_model = insightface.app.FaceAnalysis()\n",
    "face_model.prepare(ctx_id=-1, det_size=(640, 640))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import paths, resize\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_mapping = {}\n",
    "name_mapping = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DETECT_THRESHOLD = 0.6\n",
    "\n",
    "def embedding_extract(imgs_folder, id2name):\n",
    "    global id_mapping, name_mapping\n",
    "    \n",
    "    id_mapping = {}\n",
    "    name_mapping = id2name\n",
    "    count = 0\n",
    "\n",
    "    for img_path in paths.list_images(imgs_folder):\n",
    "        try:\n",
    "            dossier_id = img_path.split(os.path.sep)[-2]\n",
    "            print(f\"[DEBG] dossier_id: {dossier_id}\")\n",
    "            img = cv2.imread(img_path)\n",
    "            # TODO resize\n",
    "            img = resize(img, width=250)\n",
    "            faces = face_model.get(img)\n",
    "            if len(faces) == 1:\n",
    "                for face in faces:\n",
    "                    if face.det_score < DETECT_THRESHOLD:\n",
    "                        continue\n",
    "                    embedding = face.embedding.flatten()\n",
    "                    print(f\"[DEBUG] embedding size: {embedding.shape}\")\n",
    "                    id_mapping[str(count)] = dossier_id\n",
    "                    count += 1\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "\n",
    "    print(\"[DEBUG] extract successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBG] dossier_id: sakura\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thaivu/.pyenv/versions/3.10.2/lib/python3.10/site-packages/insightface/utils/transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] embedding size: (512,)\n",
      "[DEBG] dossier_id: sakura\n",
      "[DEBG] dossier_id: chaewon\n",
      "[DEBUG] embedding size: (512,)\n",
      "[DEBG] dossier_id: eunchae\n",
      "[DEBUG] embedding size: (512,)\n",
      "[DEBG] dossier_id: yunjin\n",
      "[DEBUG] embedding size: (512,)\n",
      "[DEBG] dossier_id: kazuha\n",
      "[DEBUG] embedding size: (512,)\n",
      "[DEBUG] extract successful\n"
     ]
    }
   ],
   "source": [
    "embedding_extract('lesserafim', 'lesserafim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'sakura', '1': 'chaewon', '2': 'eunchae', '3': 'yunjin', '4': 'kazuha'}\n",
      "lesserafim\n"
     ]
    }
   ],
   "source": [
    "print(id_mapping)\n",
    "print(name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m14:31:52\u001b[0m :: \u001b[1;35m   Helper    \u001b[0m :: \u001b[1;36m  INFO  \u001b[0m :: \u001b[1;37mRunning VidGear Version: 0.2.6\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from vidgear.gears import CamGear\n",
    "\n",
    "def analyze(video_path, sub_dir, video_id):\n",
    "    \"\"\" Analyze video\n",
    "\n",
    "    Args:\n",
    "        video_path  : video full path\n",
    "        sub_dir     : where to save dossier images found\n",
    "        video_id    : ID of video\n",
    "\n",
    "    Returns:\n",
    "        results     : List<(dossier_id/name, image_path)>\n",
    "    \"\"\"\n",
    "    print(f'[DEBUG] id_mapping: {id_mapping}')\n",
    "    results = []\n",
    "    unknown_faces = []\n",
    "\n",
    "    milestone = 5\n",
    "    extra = 5\n",
    "\n",
    "    status = 'PROCESSING'\n",
    "    # before_time = time.time()\n",
    "\n",
    "    stream = CamGear(\n",
    "        source=video_path,\n",
    "        logging=True\n",
    "    ).start()\n",
    "    frame_rate = 60\n",
    "    frame_id = 0\n",
    "    start_time = time.time()\n",
    "    completed_frames = 0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Continue retraining from video_frame_index\n",
    "            frame = stream.read()\n",
    "\n",
    "            if frame is None:\n",
    "                break\n",
    "\n",
    "            if frame_id % int(frame_rate) == 0:\n",
    "                try:\n",
    "                    completed_frames += 1\n",
    "                    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "                    faces = face_model.get(small_frame)\n",
    "                    if len(faces) > 0:\n",
    "                        timestamp = time.time() - start_time\n",
    "                        print(f\"[{completed_frames}] found {len(faces)} faces in {timestamp}\")\n",
    "                        cv2.imshow('frame', small_frame)\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord(\"q\"):\n",
    "                break   \n",
    "    finally:\n",
    "        cv2.destroyAllWindows() \n",
    "        stream.stop()\n",
    "        print(\"[INFO] finished video analysing\")\n",
    "    return results, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(\"./short_antifragile.mp4\", \"dossiers\", \"antifragile\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (main, Feb 27 2022, 23:54:06) [Clang 13.0.0 (clang-1300.0.29.3)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72cbb6b4844051e8af637830c35d5a3d1fa600669ba22fdd1774ac91ab2e3c0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
