{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimise Facial Recognition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Giới thiệu\n",
    "\n",
    "Đoạn code được trình bày trong Jupyter Notebook dưới đây gồm hai function:\n",
    "\n",
    "- `embedding_extract`: Tạo ra các embedding từ các ảnh đã được cho trước\n",
    "- `analyze`: Dựa vào các embedding được tạo ở function `embedding_extract`, analyze những khuôn mặt xuất hiện trong một video có sẵn\n",
    "\n",
    "Hai function này tương ứng với hai method cùng tên của class Analyzer, và code được trình bày trong Jupyter Notebook này gần tương tự với code trong class Analyzer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "Đầu tiên, chúng ta import một số module cần thiết cho việc vận hành của notebook này."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import insightface\n",
    "from insightface.app import FaceAnalysis\n",
    "from imutils import paths\n",
    "import cv2\n",
    "import os\n",
    "from annoy import AnnoyIndex\n",
    "import time\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "from vidgear.gears import CamGear, WriteGear\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Biến global\n",
    "\n",
    "Sau đó, chúng ta thiết lập một số biến global cần thiết như sau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/thaivu/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/thaivu/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/thaivu/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/thaivu/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/thaivu/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    }
   ],
   "source": [
    "f = 512\n",
    "annoy = AnnoyIndex(f, 'angular')\n",
    "face_model = insightface.app.FaceAnalysis()\n",
    "face_model.prepare(ctx_id=-1, det_size=(640, 640))\n",
    "face_model.models = {key: value for key, value in face_model.models.items() if key in ['detection', 'recognition']}\n",
    "id_mapping = {}\n",
    "name_mapping = \"\"\n",
    "DETECT_THRESHOLD = 0.6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở đây, sau khi thiết lập `face_model`, ta dùng một dictionary comprehension và giữ lại models detection và recognition. Việc chỉ giữ lại hai model này sẽ giúp quá trình phân tích khuân mặt trong hàm `analyze` nhanh hơn rất nhiều, vì `face_model` giờ đây chỉ cần tập trung vào việc nhận diện và nhận dạng khuôn mặt đã cho trước, thay vì cần phải phân tích cảnh hay phân tích độ tuổi / giới tính.\n",
    "\n",
    "Trong đoạn code được chứa trong `video.py`, những biến global này tương ứng với những attribute của class Analyzer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Hàm `extract_embedding`\n",
    "\n",
    "Hàm này có hai tham số truyền vào như sau:\n",
    "\n",
    "- `imgs_folder`: Thư mục chứa ảnh của những nhân vật chúng ta cần nhận dạng\n",
    "- `id2name`: Giá trị mới của `name_mapping`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_extract(imgs_folder, id2name):\n",
    "    name_mapping = id2name\n",
    "    count = 0\n",
    "\n",
    "    for img_path in tqdm(paths.list_images(imgs_folder)):\n",
    "        try:\n",
    "            dossier_id = img_path.split(os.path.sep)[-2]\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (0, 0), fx=0.25, fy=0.25)\n",
    "            faces = face_model.get(img)\n",
    "            if len(faces) == 1:\n",
    "                for face in faces:\n",
    "                    if face.det_score < DETECT_THRESHOLD:\n",
    "                        continue\n",
    "                    embedding = face.embedding.flatten()\n",
    "                    annoy.add_item(count, embedding)\n",
    "                    id_mapping[str(count)] = dossier_id\n",
    "                    count += 1\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    annoy.build(200)\n",
    "    print(\"[DEBUG] extract successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] extract successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_extract(\"hamilton\", \"hamilton\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'alex', '1': 'lin-manuel'}\n"
     ]
    }
   ],
   "source": [
    "print(id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<annoy.Annoy object at 0x13c7c3750>\n"
     ]
    }
   ],
   "source": [
    "print(annoy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm này sẽ thêm những cặp key-value vào `id_mapping`. Các cặp key-value sẽ có cấu trúc như sau:\n",
    "\n",
    "- `key`: 0, 1, 2, ...\n",
    "- `value`: Tên của những nhân vật được định dạng\n",
    "\n",
    "Để gắn tên `X` với một ảnh `p`, ta đặt `p` vào thư mục con của `imgs_folder` có tên `X`. Trong ví dụ mà ta chạy trong notebook này, ảnh của nhân vật có tên `lin-manuel` được đặt trong thư mục `hamilton/lin-manuel`, và ảnh của nhân vật có tên `alex` được đặt trong thư mục `hamilton/alex`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hàm analyze\n",
    "\n",
    "Hàm này sẽ nhận vào 4 tham số như sau:\n",
    "\n",
    "- `video_path`: đường dẫn tới video ta cần phân tích\n",
    "- `sub_dir`: thư mục chứa những ảnh dossier được tìm thấy trong quá trình phân tích video\n",
    "- `video_id`: id của video chúng ta cần phân tích, giá trị mặc định là `\"\"`\n",
    "- `outputs_video`: một boolean với giá trị mặc định là `False`. Nếu như giá trị của boolean này là `True`, ta sẽ xuất ra một video tên là output.mp4 hoặc trong thư mục outputs, với khuôn mặt của các nhân vật được bao trong một hình chữ nhật viền xanh lá cây.\n",
    "\n",
    "Hàm này sẽ trả về một list các tuple. Mỗi tuple trong hàm này có 3 phần như sau:\n",
    "\n",
    "- Tên của nhân vật được nhận dạng\n",
    "- Đường dẫn tới ảnh dossier\n",
    "- Thời gian nhân vật xuất hiện lần đầu trên màn hình\n",
    "\n",
    "Bên cạnh đó, hàm này cũng sẽ xuất ra một file `dossier.log` ghi lại thời điểm mỗi nhân vật xuất hiện và rời khỏi màn hình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(video_path, sub_dir, video_id='', outputs_video=False):\n",
    "    results = []\n",
    "\n",
    "    status = 'PROCESSING'\n",
    "\n",
    "    cap = CamGear(source=video_path).start()\n",
    "    frame_rate = cap.framerate\n",
    "\n",
    "    if outputs_video:\n",
    "        output_movie = WriteGear(output_filename=\"output.mp4\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    captured = 0\n",
    "    frame_id = 0\n",
    "    on_screen = set()\n",
    "\n",
    "    with open('dossier.log', 'w') as f:\n",
    "        try:\n",
    "            while 1:\n",
    "                frame = cap.read()\n",
    "\n",
    "                if frame is None:\n",
    "                    print('end')\n",
    "                    break\n",
    "\n",
    "                frame_id += 1\n",
    "\n",
    "                if frame_id % 5 == 0:\n",
    "                    captured += 1\n",
    "                    small_frame = cv2.resize(\n",
    "                        frame, (0, 0), fx=0.25, fy=0.25)\n",
    "                    faces = face_model.get(small_frame)\n",
    "\n",
    "                    if len(faces) <= 0:\n",
    "                        if outputs_video:\n",
    "                            output_movie.write(frame)\n",
    "                        continue\n",
    "\n",
    "                    if len(faces) > 0:\n",
    "                        timestamp = time.time() - start_time\n",
    "                        print(\n",
    "                            f'[{frame_id}] identified {len(faces)} faces at {timestamp}')\n",
    "\n",
    "                    seen_faces = set()\n",
    "\n",
    "                    for face in faces:\n",
    "                        if face.det_score < 0.6:\n",
    "                            continue\n",
    "\n",
    "                        # NOTE - bounding box for the face\n",
    "                        left, top, right, bottom = tuple(\n",
    "                            face.bbox.astype(int).flatten())\n",
    "\n",
    "                        embedding = face.embedding.flatten()\n",
    "                        closest_indices, distances = annoy.get_nns_by_vector(\n",
    "                            embedding, n=1, include_distances=True)\n",
    "                        similarity = (2. - distances[0] ** 2) / 2.\n",
    "\n",
    "                        if similarity >= 0.2:\n",
    "                            crop_img = small_frame[top:bottom, left:right]\n",
    "                            dossier_id = id_mapping[str(\n",
    "                                closest_indices[0])]\n",
    "                            seen_faces.add(dossier_id)\n",
    "\n",
    "                            if dossier_id not in on_screen:\n",
    "                                print(\n",
    "                                    f'{dossier_id} on screen since {frame_id / frame_rate}', file=f)\n",
    "                                on_screen.add(dossier_id)\n",
    "                                filename = dossier_id + '_' + \\\n",
    "                                    str(uuid.uuid4()) + '.jpg'\n",
    "                                path_to_save = os.path.join(\n",
    "                                    sub_dir, filename)\n",
    "                                writer = WriteGear(\n",
    "                                    output_filename=path_to_save, compression_mode=False)\n",
    "                                writer.write(crop_img)\n",
    "                                timecode = frame_id / frame_rate\n",
    "                                results.append((dossier_id, path_to_save, timecode))\n",
    "\n",
    "                            if outputs_video:\n",
    "                                cv2.rectangle(frame, (left * 4, top * 4),\n",
    "                                                (right * 4, bottom * 4), (0, 255, 0), 2)\n",
    "                                cv2.rectangle(frame, (left * 4, bottom * 4 + 10),\n",
    "                                                (right * 4, bottom * 4), (0, 255, 0), cv2.FILLED)\n",
    "                                font = cv2.FONT_HERSHEY_DUPLEX\n",
    "                                cv2.putText(frame, dossier_id, (left * 4 + 6, bottom * 4 - 6),\n",
    "                                            font, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "                    for person in on_screen:\n",
    "                        if person not in seen_faces:\n",
    "                            print(\n",
    "                                f'{person} left screen at {frame_id / frame_rate}', file=f)\n",
    "                            # del tagged_trackers[person]\n",
    "\n",
    "                    on_screen = seen_faces\n",
    "\n",
    "                    if outputs_video:\n",
    "                        output_movie.write(frame)\n",
    "        finally:\n",
    "            cap.stop()\n",
    "\n",
    "            if outputs_video:\n",
    "                output_movie.close()\n",
    "                \n",
    "            analysis_length = time.time() - start_time\n",
    "            print(f\"[INFO] finished video analysis in {analysis_length}\")\n",
    "\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m16:17:30\u001b[0m :: \u001b[1;35m  WriteGear  \u001b[0m :: \u001b[1;31m\u001b[47mCRITICAL\u001b[0m :: \u001b[1;37mCompression Mode is disabled, Activating OpenCV built-in Writer!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5] identified 2 faces at 0.7156069278717041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x47504a4d/'MJPG' is not supported with codec id 7 and format 'image2 / image2 sequence'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10] identified 2 faces at 1.3791148662567139\n",
      "[15] identified 2 faces at 2.287763833999634\n",
      "[20] identified 1 faces at 2.8923590183258057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m16:17:33\u001b[0m :: \u001b[1;35m  WriteGear  \u001b[0m :: \u001b[1;31m\u001b[47mCRITICAL\u001b[0m :: \u001b[1;37mCompression Mode is disabled, Activating OpenCV built-in Writer!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25] identified 3 faces at 3.6839189529418945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x47504a4d/'MJPG' is not supported with codec id 7 and format 'image2 / image2 sequence'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30] identified 2 faces at 4.322631120681763\n",
      "[35] identified 2 faces at 4.859760999679565\n",
      "[40] identified 1 faces at 5.284618854522705\n",
      "[45] identified 3 faces at 5.978823184967041\n",
      "[50] identified 2 faces at 6.653105020523071\n",
      "[55] identified 4 faces at 7.609364986419678\n",
      "[60] identified 6 faces at 9.079757928848267\n",
      "[65] identified 6 faces at 10.388646125793457\n",
      "[70] identified 5 faces at 11.360676050186157\n",
      "[75] identified 2 faces at 11.878797054290771\n",
      "[80] identified 4 faces at 12.674093961715698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m16:17:42\u001b[0m :: \u001b[1;35m  WriteGear  \u001b[0m :: \u001b[1;31m\u001b[47mCRITICAL\u001b[0m :: \u001b[1;37mCompression Mode is disabled, Activating OpenCV built-in Writer!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85] identified 1 faces at 13.034674882888794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x47504a4d/'MJPG' is not supported with codec id 7 and format 'image2 / image2 sequence'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90] identified 1 faces at 13.434889793395996\n",
      "[95] identified 1 faces at 13.803447008132935\n",
      "[100] identified 1 faces at 14.191295862197876\n",
      "[105] identified 1 faces at 14.555557012557983\n",
      "[110] identified 1 faces at 15.03726601600647\n",
      "[115] identified 1 faces at 15.402453899383545\n",
      "[120] identified 1 faces at 15.958757162094116\n",
      "[125] identified 1 faces at 16.421425819396973\n",
      "[130] identified 1 faces at 16.772372007369995\n",
      "[135] identified 1 faces at 17.15442991256714\n",
      "[140] identified 1 faces at 17.52244806289673\n",
      "[145] identified 1 faces at 17.89653706550598\n",
      "[150] identified 1 faces at 18.259632110595703\n",
      "[155] identified 1 faces at 18.62764310836792\n",
      "[160] identified 1 faces at 18.999907970428467\n",
      "[165] identified 1 faces at 19.388819932937622\n",
      "[170] identified 1 faces at 19.766921997070312\n",
      "[175] identified 1 faces at 20.151077032089233\n",
      "[180] identified 1 faces at 20.55998992919922\n",
      "[185] identified 1 faces at 21.018802881240845\n",
      "[190] identified 1 faces at 21.498097896575928\n",
      "[195] identified 1 faces at 21.98465895652771\n",
      "[200] identified 1 faces at 22.45651912689209\n",
      "[205] identified 1 faces at 22.96227502822876\n",
      "[210] identified 1 faces at 23.389026880264282\n",
      "[215] identified 4 faces at 24.289800882339478\n",
      "[220] identified 4 faces at 25.69977378845215\n",
      "[225] identified 5 faces at 26.696606874465942\n",
      "[230] identified 5 faces at 27.655250787734985\n",
      "[235] identified 5 faces at 28.637213945388794\n",
      "[240] identified 5 faces at 29.69134211540222\n",
      "[245] identified 4 faces at 30.52670693397522\n",
      "[250] identified 4 faces at 31.452111959457397\n",
      "[255] identified 4 faces at 32.310141801834106\n",
      "[260] identified 4 faces at 33.17430281639099\n",
      "[265] identified 4 faces at 34.09879493713379\n",
      "[270] identified 4 faces at 34.97191381454468\n",
      "[275] identified 4 faces at 35.75081205368042\n",
      "end\n",
      "[INFO] finished video analysis in 35.75202703475952\n",
      "[('lin-manuel', 'dossier/lin-manuel_7519b655-0036-4cf9-aa59-a15d54fe66e5.jpg', 0.16683350016683351), ('alex', 'dossier/alex_523ba256-d7ab-45fc-b6f4-f6542e2f56bd.jpg', 0.8341675008341676), ('lin-manuel', 'dossier/lin-manuel_9ba84b27-3f3b-4f8b-ae48-a9c7ae58bc14.jpg', 2.8361695028361695)]\n"
     ]
    }
   ],
   "source": [
    "print(analyze('short_hamilton_clip.mp4', 'dossier', 'hamilton', outputs_video=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72cbb6b4844051e8af637830c35d5a3d1fa600669ba22fdd1774ac91ab2e3c0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
