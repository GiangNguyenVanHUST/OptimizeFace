{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimise Facial Recognition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Giới thiệu\n",
    "\n",
    "Đoạn code được trình bày trong Jupyter Notebook dưới đây gồm hai function:\n",
    "\n",
    "- `embedding_extract`: Tạo ra các embedding từ các ảnh đã được cho trước\n",
    "- `analyze`: Dựa vào các embedding được tạo ở function `embedding_extract`, analyze những khuôn mặt xuất hiện trong một video có sẵn\n",
    "\n",
    "Hai function này tương ứng với hai method cùng tên của class Analyzer, và code được trình bày trong Jupyter Notebook này gần tương tự với code trong class Analyzer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "Đầu tiên, chúng ta import một số module cần thiết cho việc vận hành của notebook này."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import insightface\n",
    "from insightface.app import FaceAnalysis\n",
    "from imutils import paths\n",
    "import cv2\n",
    "import os\n",
    "from annoy import AnnoyIndex\n",
    "import time\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "from vidgear.gears import CamGear, WriteGear\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Biến global\n",
    "\n",
    "Sau đó, chúng ta thiết lập một số biến global cần thiết như sau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/thaivu/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/thaivu/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/thaivu/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/thaivu/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/thaivu/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    }
   ],
   "source": [
    "f = 512\n",
    "annoy = AnnoyIndex(f, 'angular')\n",
    "face_model = insightface.app.FaceAnalysis()\n",
    "face_model.prepare(ctx_id=-1, det_size=(640, 640))\n",
    "face_model.models = {key: value for key, value in face_model.models.items() if key in ['detection', 'recognition']}\n",
    "id_mapping = {}\n",
    "name_mapping = \"\"\n",
    "DETECT_THRESHOLD = 0.6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở đây, sau khi thiết lập `face_model`, ta dùng một dictionary comprehension và giữ lại models detection và recognition. Việc chỉ giữ lại hai model này sẽ giúp quá trình phân tích khuân mặt trong hàm `analyze` nhanh hơn rất nhiều, vì `face_model` giờ đây chỉ cần tập trung vào việc nhận diện và nhận dạng khuôn mặt đã cho trước, thay vì cần phải phân tích cảnh hay phân tích độ tuổi / giới tính.\n",
    "\n",
    "Trong đoạn code được chứa trong `video.py`, những biến global này tương ứng với những attribute của class Analyzer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Hàm `extract_embedding`\n",
    "\n",
    "Hàm này có hai tham số truyền vào như sau:\n",
    "\n",
    "- `imgs_folder`: Thư mục chứa ảnh của những nhân vật chúng ta cần nhận dạng\n",
    "- `id2name`: Giá trị mới của `name_mapping`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_extract(imgs_folder, id2name):\n",
    "    name_mapping = id2name\n",
    "    count = 0\n",
    "\n",
    "    for img_path in tqdm(paths.list_images(imgs_folder)):\n",
    "        try:\n",
    "            dossier_id = img_path.split(os.path.sep)[-2]\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (0, 0), fx=0.25, fy=0.25)\n",
    "            faces = face_model.get(img)\n",
    "            if len(faces) == 1:\n",
    "                for face in faces:\n",
    "                    if face.det_score < DETECT_THRESHOLD:\n",
    "                        continue\n",
    "                    embedding = face.embedding.flatten()\n",
    "                    annoy.add_item(count, embedding)\n",
    "                    id_mapping[str(count)] = dossier_id\n",
    "                    count += 1\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    annoy.build(200)\n",
    "    print(\"[DEBUG] extract successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] extract successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_extract(\"input/hamilton\", \"hamilton\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'alex', '1': 'lin-manuel'}\n"
     ]
    }
   ],
   "source": [
    "print(id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<annoy.Annoy object at 0x13c7c3750>\n"
     ]
    }
   ],
   "source": [
    "print(annoy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm này sẽ thêm những cặp key-value vào `id_mapping`. Các cặp key-value sẽ có cấu trúc như sau:\n",
    "\n",
    "- `key`: 0, 1, 2, ...\n",
    "- `value`: Tên của những nhân vật được định dạng\n",
    "\n",
    "Để gắn tên `X` với một ảnh `p`, ta đặt `p` vào thư mục con của `imgs_folder` có tên `X`. Trong ví dụ mà ta chạy trong notebook này, ảnh của nhân vật có tên `lin-manuel` được đặt trong thư mục `hamilton/lin-manuel`, và ảnh của nhân vật có tên `alex` được đặt trong thư mục `hamilton/alex`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hàm analyze\n",
    "\n",
    "Hàm này sẽ nhận vào 5 tham số như sau:\n",
    "\n",
    "- `video_path`: đường dẫn tới video ta cần phân tích\n",
    "- `sub_dir`: thư mục chứa những ảnh dossier được tìm thấy trong quá trình phân tích video\n",
    "- `video_id`: id của video chúng ta cần phân tích, giá trị mặc định là `\"\"`\n",
    "- `outputs_video`: một boolean với giá trị mặc định là `False`. Nếu như giá trị của boolean này là `True`, ta sẽ xuất ra một video tên là output.mp4 (khi chạy từ notebook này) hoặc trong thư mục outputs (khi chạy từ method `analyze` của class `Analyzer`), với khuôn mặt của các nhân vật được bao trong một hình chữ nhật viền xanh lá cây. \n",
    "- `output_folder`: một string với giá trị mặc định là None. Sau khi video output được tạo xong, video output đó sẽ được lưu tại thư mục này. Trong trường hợp biến này có giá trị là None, tên của thư mục sẽ có giá trị mặc định là `output`.\n",
    "\n",
    "Hàm này sẽ trả về một list các tuple. Mỗi tuple trong hàm này có 3 phần như sau:\n",
    "\n",
    "- Tên của nhân vật được nhận dạng\n",
    "- Đường dẫn tới ảnh dossier\n",
    "- Thời gian nhân vật xuất hiện lần đầu trên màn hình\n",
    "\n",
    "Bên cạnh đó, hàm này cũng sẽ xuất ra một file `dossier.log` ghi lại thời điểm mỗi nhân vật xuất hiện và rời khỏi màn hình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(video_path, sub_dir, video_id='', outputs_video=False, output_folder=None):\n",
    "    results = []\n",
    "\n",
    "    status = 'PROCESSING'\n",
    "\n",
    "    cap = CamGear(source=video_path).start()\n",
    "    frame_rate = cap.framerate\n",
    "\n",
    "    output_destination = output_folder if output_folder else 'output'\n",
    "\n",
    "    if not os.path.exists(output_destination):\n",
    "        os.mkdir(output_destination)\n",
    "\n",
    "    if outputs_video:\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        video_file = video_path.split('/')[-1]\n",
    "        video_name_tokens = video_file.split('.')[:-1]\n",
    "        video_name = \".\".join(video_name_tokens)\n",
    "        print('video_name:', video_name)\n",
    "\n",
    "        output_movie = WriteGear(\n",
    "            output_filename=f\"./{output_destination}/{video_name}_output.mp4\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    captured = 0\n",
    "    frame_id = 0\n",
    "    on_screen = set()\n",
    "\n",
    "    with open('dossier.log', 'w') as f:\n",
    "        try:\n",
    "            while 1:\n",
    "                frame = cap.read()\n",
    "\n",
    "                if frame is None:\n",
    "                    print('end')\n",
    "                    break\n",
    "\n",
    "                frame_id += 1\n",
    "\n",
    "                if frame_id % 5 == 0:\n",
    "                    captured += 1\n",
    "                    small_frame = cv2.resize(\n",
    "                        frame, (0, 0), fx=0.25, fy=0.25)\n",
    "                    faces = face_model.get(small_frame)\n",
    "\n",
    "                    if len(faces) <= 0:\n",
    "                        if outputs_video:\n",
    "                            output_movie.write(frame)\n",
    "                        continue\n",
    "\n",
    "                    if len(faces) > 0:\n",
    "                        timestamp = time.time() - start_time\n",
    "                        print(\n",
    "                            f'[{frame_id}] identified {len(faces)} faces at {timestamp}')\n",
    "\n",
    "                    seen_faces = set()\n",
    "\n",
    "                    for face in faces:\n",
    "                        if face.det_score < 0.6:\n",
    "                            continue\n",
    "\n",
    "                        # NOTE - bounding box for the face\n",
    "                        left, top, right, bottom = tuple(\n",
    "                            face.bbox.astype(int).flatten())\n",
    "\n",
    "                        embedding = face.embedding.flatten()\n",
    "                        annoy_start = time.time()\n",
    "                        closest_indices, distances = annoy.get_nns_by_vector(\n",
    "                            embedding, n=1, include_distances=True)\n",
    "                        annoy_end = time.time()\n",
    "                        print(\n",
    "                            f\"[{frame_id}] annoy finished in {annoy_end - annoy_start}\")\n",
    "                        similarity = (2. - distances[0] ** 2) / 2.\n",
    "\n",
    "                        if similarity >= 0.2:\n",
    "                            crop_img = small_frame[top:bottom, left:right]\n",
    "                            dossier_id = id_mapping[str(\n",
    "                                closest_indices[0])]\n",
    "                            seen_faces.add(dossier_id)\n",
    "\n",
    "                            if dossier_id not in on_screen:\n",
    "                                print(\n",
    "                                    f'{dossier_id} on screen since {frame_id / frame_rate}', file=f)\n",
    "                                on_screen.add(dossier_id)\n",
    "                                filename = dossier_id + '_' + \\\n",
    "                                    str(uuid.uuid4()) + '.jpg'\n",
    "                                path_to_save = os.path.join(\n",
    "                                    sub_dir, filename)\n",
    "                                writer = WriteGear(\n",
    "                                    output_filename=path_to_save, compression_mode=False)\n",
    "                                writer.write(crop_img)\n",
    "                                timecode = frame_id / frame_rate\n",
    "                                results.append((dossier_id, path_to_save, timecode))\n",
    "\n",
    "                            if outputs_video:\n",
    "                                cv2.rectangle(frame, (left * 4, top * 4),\n",
    "                                                (right * 4, bottom * 4), (0, 255, 0), 2)\n",
    "                                cv2.rectangle(frame, (left * 4, bottom * 4 + 10),\n",
    "                                                (right * 4, bottom * 4), (0, 255, 0), cv2.FILLED)\n",
    "                                font = cv2.FONT_HERSHEY_DUPLEX\n",
    "                                cv2.putText(frame, dossier_id, (left * 4 + 6, bottom * 4 - 6),\n",
    "                                            font, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "                    for person in on_screen:\n",
    "                        if person not in seen_faces:\n",
    "                            print(\n",
    "                                f'{person} left screen at {frame_id / frame_rate}', file=f)\n",
    "                            # del tagged_trackers[person]\n",
    "\n",
    "                    on_screen = seen_faces\n",
    "\n",
    "                    if outputs_video:\n",
    "                        output_movie.write(frame)\n",
    "        finally:\n",
    "            cap.stop()\n",
    "\n",
    "            if outputs_video:\n",
    "                output_movie.close()\n",
    "                \n",
    "            analysis_length = time.time() - start_time\n",
    "            print(f\"[INFO] finished video analysis in {analysis_length}\")\n",
    "\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m16:17:30\u001b[0m :: \u001b[1;35m  WriteGear  \u001b[0m :: \u001b[1;31m\u001b[47mCRITICAL\u001b[0m :: \u001b[1;37mCompression Mode is disabled, Activating OpenCV built-in Writer!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5] identified 2 faces at 0.7156069278717041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x47504a4d/'MJPG' is not supported with codec id 7 and format 'image2 / image2 sequence'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10] identified 2 faces at 1.3791148662567139\n",
      "[15] identified 2 faces at 2.287763833999634\n",
      "[20] identified 1 faces at 2.8923590183258057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m16:17:33\u001b[0m :: \u001b[1;35m  WriteGear  \u001b[0m :: \u001b[1;31m\u001b[47mCRITICAL\u001b[0m :: \u001b[1;37mCompression Mode is disabled, Activating OpenCV built-in Writer!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25] identified 3 faces at 3.6839189529418945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x47504a4d/'MJPG' is not supported with codec id 7 and format 'image2 / image2 sequence'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30] identified 2 faces at 4.322631120681763\n",
      "[35] identified 2 faces at 4.859760999679565\n",
      "[40] identified 1 faces at 5.284618854522705\n",
      "[45] identified 3 faces at 5.978823184967041\n",
      "[50] identified 2 faces at 6.653105020523071\n",
      "[55] identified 4 faces at 7.609364986419678\n",
      "[60] identified 6 faces at 9.079757928848267\n",
      "[65] identified 6 faces at 10.388646125793457\n",
      "[70] identified 5 faces at 11.360676050186157\n",
      "[75] identified 2 faces at 11.878797054290771\n",
      "[80] identified 4 faces at 12.674093961715698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m16:17:42\u001b[0m :: \u001b[1;35m  WriteGear  \u001b[0m :: \u001b[1;31m\u001b[47mCRITICAL\u001b[0m :: \u001b[1;37mCompression Mode is disabled, Activating OpenCV built-in Writer!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85] identified 1 faces at 13.034674882888794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x47504a4d/'MJPG' is not supported with codec id 7 and format 'image2 / image2 sequence'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90] identified 1 faces at 13.434889793395996\n",
      "[95] identified 1 faces at 13.803447008132935\n",
      "[100] identified 1 faces at 14.191295862197876\n",
      "[105] identified 1 faces at 14.555557012557983\n",
      "[110] identified 1 faces at 15.03726601600647\n",
      "[115] identified 1 faces at 15.402453899383545\n",
      "[120] identified 1 faces at 15.958757162094116\n",
      "[125] identified 1 faces at 16.421425819396973\n",
      "[130] identified 1 faces at 16.772372007369995\n",
      "[135] identified 1 faces at 17.15442991256714\n",
      "[140] identified 1 faces at 17.52244806289673\n",
      "[145] identified 1 faces at 17.89653706550598\n",
      "[150] identified 1 faces at 18.259632110595703\n",
      "[155] identified 1 faces at 18.62764310836792\n",
      "[160] identified 1 faces at 18.999907970428467\n",
      "[165] identified 1 faces at 19.388819932937622\n",
      "[170] identified 1 faces at 19.766921997070312\n",
      "[175] identified 1 faces at 20.151077032089233\n",
      "[180] identified 1 faces at 20.55998992919922\n",
      "[185] identified 1 faces at 21.018802881240845\n",
      "[190] identified 1 faces at 21.498097896575928\n",
      "[195] identified 1 faces at 21.98465895652771\n",
      "[200] identified 1 faces at 22.45651912689209\n",
      "[205] identified 1 faces at 22.96227502822876\n",
      "[210] identified 1 faces at 23.389026880264282\n",
      "[215] identified 4 faces at 24.289800882339478\n",
      "[220] identified 4 faces at 25.69977378845215\n",
      "[225] identified 5 faces at 26.696606874465942\n",
      "[230] identified 5 faces at 27.655250787734985\n",
      "[235] identified 5 faces at 28.637213945388794\n",
      "[240] identified 5 faces at 29.69134211540222\n",
      "[245] identified 4 faces at 30.52670693397522\n",
      "[250] identified 4 faces at 31.452111959457397\n",
      "[255] identified 4 faces at 32.310141801834106\n",
      "[260] identified 4 faces at 33.17430281639099\n",
      "[265] identified 4 faces at 34.09879493713379\n",
      "[270] identified 4 faces at 34.97191381454468\n",
      "[275] identified 4 faces at 35.75081205368042\n",
      "end\n",
      "[INFO] finished video analysis in 35.75202703475952\n",
      "[('lin-manuel', 'dossier/lin-manuel_7519b655-0036-4cf9-aa59-a15d54fe66e5.jpg', 0.16683350016683351), ('alex', 'dossier/alex_523ba256-d7ab-45fc-b6f4-f6542e2f56bd.jpg', 0.8341675008341676), ('lin-manuel', 'dossier/lin-manuel_9ba84b27-3f3b-4f8b-ae48-a9c7ae58bc14.jpg', 2.8361695028361695)]\n"
     ]
    }
   ],
   "source": [
    "print(analyze('input/short_hamilton_clip.mp4', 'dossier', 'hamilton', outputs_video=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sử dụng với module FaceAnalyzer\n",
    "\n",
    "Hai function `extract_embedding` và `analyze` ở trên tương ứng với hai method trong class Analyzer, chứa trong module FaceAnalyzer.\n",
    "\n",
    "Bạn có thể import class Analyzer và khởi tạo class này với câu lệnh sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/thaivu/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/thaivu/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/thaivu/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/thaivu/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /Users/thaivu/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    }
   ],
   "source": [
    "from FaceAnalyzer import Analyzer\n",
    "\n",
    "analyzer = Analyzer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để thực hiện những thao tác tương tự như hai function ở trên, bạn có thể sử dụng những câu lệnh như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] extract successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "analyzer.embedding_extract(\"hamilton\", \"hamilton\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m10:58:33\u001b[0m :: \u001b[1;35m  WriteGear  \u001b[0m :: \u001b[1;31m\u001b[47mCRITICAL\u001b[0m :: \u001b[1;37mCompression Mode is disabled, Activating OpenCV built-in Writer!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5] identified 2 faces at 0.4110867977142334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x47504a4d/'MJPG' is not supported with codec id 7 and format 'image2 / image2 sequence'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10] identified 2 faces at 0.8099279403686523\n",
      "[15] identified 2 faces at 1.1508910655975342\n",
      "[20] identified 1 faces at 1.4128968715667725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m10:58:34\u001b[0m :: \u001b[1;35m  WriteGear  \u001b[0m :: \u001b[1;31m\u001b[47mCRITICAL\u001b[0m :: \u001b[1;37mCompression Mode is disabled, Activating OpenCV built-in Writer!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25] identified 3 faces at 1.8555388450622559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x47504a4d/'MJPG' is not supported with codec id 7 and format 'image2 / image2 sequence'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30] identified 2 faces at 2.2328410148620605\n",
      "[35] identified 2 faces at 2.6260039806365967\n",
      "[40] identified 1 faces at 2.9521169662475586\n",
      "[45] identified 3 faces at 3.4372878074645996\n",
      "[50] identified 2 faces at 3.777549982070923\n",
      "[55] identified 4 faces at 4.3036417961120605\n",
      "[60] identified 6 faces at 5.110954999923706\n",
      "[INFO] finished video analysis in 6.094574689865112\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/thaivu/.pyenv/versions/3.10.2/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/_x/zckfnlk92vq72rvw9szdlzqw0000gn/T/ipykernel_7624/2371828781.py\", line 1, in <module>\n",
      "    print(analyzer.analyze('short_hamilton_clip.mp4', 'dossier', 'hamilton', outputs_video=False))\n",
      "  File \"/Users/thaivu/Documents/vsi-intern/learn-insightface/FaceAnalyzer.py\", line 104, in analyze\n",
      "  File \"/Users/thaivu/.pyenv/versions/3.10.2/lib/python3.10/site-packages/insightface/app/face_analysis.py\", line 75, in get\n",
      "    model.get(img, face)\n",
      "  File \"/Users/thaivu/.pyenv/versions/3.10.2/lib/python3.10/site-packages/insightface/model_zoo/arcface_onnx.py\", line 67, in get\n",
      "    face.embedding = self.get_feat(aimg).flatten()\n",
      "  File \"/Users/thaivu/.pyenv/versions/3.10.2/lib/python3.10/site-packages/insightface/model_zoo/arcface_onnx.py\", line 84, in get_feat\n",
      "    net_out = self.session.run(self.output_names, {self.input_name: blob})[0]\n",
      "  File \"/Users/thaivu/.pyenv/versions/3.10.2/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\", line 200, in run\n",
      "    return self._sess.run(output_names, input_feed, run_options)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/thaivu/.pyenv/versions/3.10.2/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2052, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/thaivu/.pyenv/versions/3.10.2/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/thaivu/.pyenv/versions/3.10.2/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/thaivu/.pyenv/versions/3.10.2/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/thaivu/.pyenv/versions/3.10.2/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/Users/thaivu/.pyenv/versions/3.10.2/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/Users/thaivu/.pyenv/versions/3.10.2/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/thaivu/.pyenv/versions/3.10.2/lib/python3.10/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/thaivu/.pyenv/versions/3.10.2/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/thaivu/.pyenv/versions/3.10.2/lib/python3.10/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/thaivu/.pyenv/versions/3.10.2/lib/python3.10/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/thaivu/.pyenv/versions/3.10.2/lib/python3.10/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/thaivu/.pyenv/versions/3.10.2/lib/python3.10/site-packages/executing/executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "print(analyzer.analyze('short_hamilton_clip.mp4', 'dossier', 'hamilton', outputs_video=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (main, Feb 27 2022, 23:54:06) [Clang 13.0.0 (clang-1300.0.29.3)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72cbb6b4844051e8af637830c35d5a3d1fa600669ba22fdd1774ac91ab2e3c0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
